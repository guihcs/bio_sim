{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from rdflib import Graph\n",
    "from rdflib.term import URIRef, BNode, Literal\n",
    "from rdflib.namespace import RDF, RDFS, OWL\n",
    "from om.ont import get_n, tokenize\n",
    "from termcolor import colored\n",
    "from ldp import parser\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def print_node(n, g, l=0, ml=1):\n",
    "    print('\\t' * l, get_n(n, g))\n",
    "    if l >= ml:\n",
    "        return\n",
    "    for _, p, o in g.triples((n, None, None)):\n",
    "        print('\\t' * (l + 1), colored(get_n(p, g), 'blue'), end=' ')\n",
    "        print_node(o, g, l+1, ml)\n",
    "\n",
    "def get_ld(ld, g, res, l=0):\n",
    "\n",
    "    if type(ld) is tuple:\n",
    "        for c in ld[1]:\n",
    "            get_ld(c, g, res, l+1)\n",
    "\n",
    "\n",
    "    elif type(ld) is URIRef:\n",
    "        res.append(str(g.value(ld, RDFS.label)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/24429 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dee74a68241e49229d13837fe5d92404"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('logdefs_HP.csv', 'r') as f:\n",
    "    raw_defs = list(map(lambda x: x.split(','), f.readlines()))\n",
    "\n",
    "ldf = []\n",
    "for ld in tqdm(raw_defs):\n",
    "\n",
    "    if len(ld) > 2:\n",
    "        continue\n",
    "    line = []\n",
    "    try:\n",
    "        ldf.append((URIRef(ld[0]), parser.parse(ld[1])))\n",
    "    except:\n",
    "        continue"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "hp = Graph().parse('hp.owl')\n",
    "\n",
    "x = []\n",
    "c = []\n",
    "\n",
    "for e, l in ldf:\n",
    "\n",
    "    e1l = hp.value(e, RDFS.label)\n",
    "\n",
    "    ld = []\n",
    "    get_ld(l, hp, ld)\n",
    "\n",
    "    x.append(e1l)\n",
    "    c.append(ld)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24425\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, c):\n",
    "        self.transform = None\n",
    "        self.target_transform = None\n",
    "\n",
    "        self.x = x\n",
    "        self.c = c\n",
    "\n",
    "\n",
    "        ml = -1\n",
    "        for q in itertools.chain(x, itertools.chain(*c)):\n",
    "            l = len(tokenizer.tokenize(q))\n",
    "            if l > ml:\n",
    "                ml = l\n",
    "        self.ml = ml\n",
    "        self.ms = max(map(len, c))\n",
    "\n",
    "        tk1 = tokenizer(x, return_tensors='pt', padding='max_length', max_length=ml)\n",
    "        self.id1 = tk1['input_ids']\n",
    "        self.at1 = tk1['attention_mask']\n",
    "\n",
    "        tc = []\n",
    "        tm = {}\n",
    "        for q in c:\n",
    "\n",
    "            tk1 = tokenizer(q, return_tensors='pt', padding='max_length', truncation=True, max_length=ml)\n",
    "\n",
    "            tc.append((tk1['input_ids'], tk1['attention_mask']))\n",
    "            for w, a in zip(tk1['input_ids'], tk1['attention_mask']):\n",
    "                tm[w] = a\n",
    "\n",
    "        self.tc = tc\n",
    "        self.tm = tm\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.id1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id1 = self.id1[idx]\n",
    "        at1 = self.at1[idx]\n",
    "\n",
    "        tc1, ta1 = self.tc[idx]\n",
    "        ns = random.choices(list(self.tm.keys()), k=tc1.shape[0])\n",
    "        nat = list(map(lambda x: self.tm[x], ns))\n",
    "\n",
    "        ns = torch.cat(list(map(lambda x: x.unsqueeze(0), ns)), dim=0)\n",
    "        nat = torch.cat(list(map(lambda x: x.unsqueeze(0), nat)), dim=0)\n",
    "\n",
    "        sm = [1] * tc1.shape[0]\n",
    "\n",
    "        if tc1.shape[0] < self.ms:\n",
    "            dif = self.ms - tc1.shape[0]\n",
    "            sm.extend([0] * dif)\n",
    "            pad = torch.zeros((dif, self.ml))\n",
    "            tc1 = torch.cat([tc1, pad], dim=0)\n",
    "            ta1 = torch.cat([ta1, pad], dim=0)\n",
    "            ns = torch.cat([ns, pad], dim=0)\n",
    "            nat = torch.cat([nat, pad], dim=0)\n",
    "\n",
    "        sm = torch.Tensor(sm)\n",
    "        return (id1, at1), (tc1, ta1), (ns, nat), sm\n",
    "\n",
    "\n",
    "\n",
    "dataset = CustomDataset(x, c)\n",
    "\n",
    "print(len(dataset))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "        self.biobert = AutoModel.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "        for param in self.biobert.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.biobert.eval()\n",
    "\n",
    "        self.dff = nn.Sequential(\n",
    "            nn.Linear(768 * 2, 768 * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(768 * 4, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def emb(self, x, a):\n",
    "        out = self.biobert(input_ids=x, attention_mask=a)['last_hidden_state']\n",
    "        cf = a.sum(dim=1, keepdims=True)\n",
    "        cf[cf == 0] = 1\n",
    "        res = out.sum(1) / cf\n",
    "        return res\n",
    "\n",
    "    def forward(self, x, xa, y, ya):\n",
    "        e1 = self.emb(x, xa)\n",
    "        bs = y.shape[0]\n",
    "        ss = y.shape[1]\n",
    "\n",
    "        e2 = self.emb(torch.flatten(y, end_dim=1).long(), torch.flatten(ya, end_dim=1))\n",
    "\n",
    "        jc = torch.cat([e1.unsqueeze(1).repeat(1, ss, 1), e2.reshape(bs, ss, -1)], dim=2)\n",
    "\n",
    "        return self.dff(jc)\n",
    "\n",
    "\n",
    "    def sims(self, anc, ex, bs=10):\n",
    "        at = self.tokenizer([anc] + ex, return_tensors='pt', padding=True)\n",
    "\n",
    "\n",
    "        ids = at['input_ids']\n",
    "        ats = at['attention_mask']\n",
    "\n",
    "        act = ids[0]\n",
    "        aca = ats[0]\n",
    "\n",
    "        ext = ids[1:]\n",
    "        exa = ats[1:]\n",
    "\n",
    "\n",
    "        os = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for e, a in DataLoader(list(zip(ext, exa)), batch_size=bs):\n",
    "                out = self(act.unsqueeze(0), aca.unsqueeze(0), e.unsqueeze(0), a.unsqueeze(0))\n",
    "                os.append(out.squeeze(0).t().squeeze(0))\n",
    "\n",
    "        return torch.cat(os, dim=0)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/22920 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "555213ea5d944683aaec14123d545122"
      },
      "application/json": {
       "n": 0,
       "total": 22920,
       "elapsed": 0.012302160263061523,
       "ncols": null,
       "nrows": null,
       "prefix": "",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [20]\u001B[0m, in \u001B[0;36m<cell line: 12>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     17\u001B[0m ep \u001B[38;5;241m=\u001B[39m model(ids\u001B[38;5;241m.\u001B[39mcuda(\u001B[38;5;241m0\u001B[39m), ati\u001B[38;5;241m.\u001B[39mcuda(\u001B[38;5;241m0\u001B[39m), ps\u001B[38;5;241m.\u001B[39mcuda(\u001B[38;5;241m0\u001B[39m), pa\u001B[38;5;241m.\u001B[39mcuda(\u001B[38;5;241m0\u001B[39m))\n\u001B[1;32m     18\u001B[0m en \u001B[38;5;241m=\u001B[39m model(ids\u001B[38;5;241m.\u001B[39mcuda(\u001B[38;5;241m0\u001B[39m), ati\u001B[38;5;241m.\u001B[39mcuda(\u001B[38;5;241m0\u001B[39m), ns\u001B[38;5;241m.\u001B[39mcuda(\u001B[38;5;241m0\u001B[39m), na\u001B[38;5;241m.\u001B[39mcuda(\u001B[38;5;241m0\u001B[39m))\n\u001B[0;32m---> 21\u001B[0m pv \u001B[38;5;241m=\u001B[39m ep \u001B[38;5;241m*\u001B[39m \u001B[43msm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m (sm \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mcuda(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     22\u001B[0m nv \u001B[38;5;241m=\u001B[39m en \u001B[38;5;241m*\u001B[39m sm\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mcuda(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     24\u001B[0m ploss \u001B[38;5;241m=\u001B[39m crit(pv, torch\u001B[38;5;241m.\u001B[39mones(sm\u001B[38;5;241m.\u001B[39mshape)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mcuda(\u001B[38;5;241m0\u001B[39m))\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = nn.DataParallel(Model())\n",
    "model.cuda(0)\n",
    "crit = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.00003)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "\n",
    "progress = tqdm(total=epochs * math.ceil(len(dataset) / batch_size))\n",
    "\n",
    "lh = []\n",
    "for e in range(epochs):\n",
    "    el = []\n",
    "    for (ids, ati), (ps, pa), (ns, na), sm in DataLoader(dataset, batch_size=batch_size, shuffle=True):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ep = model(ids.cuda(0), ati.cuda(0), ps.cuda(0), pa.cuda(0))\n",
    "        en = model(ids.cuda(0), ati.cuda(0), ns.cuda(0), na.cuda(0))\n",
    "\n",
    "\n",
    "        pv = ep * sm.unsqueeze(-1).cuda(0) + (sm == 0).float().unsqueeze(-1).cuda(0)\n",
    "        nv = en * sm.unsqueeze(-1).cuda(0)\n",
    "\n",
    "        ploss = crit(pv, torch.ones(sm.shape).unsqueeze(-1).cuda(0))\n",
    "        nloss = crit(nv, torch.zeros(sm.shape).unsqueeze(-1).cuda(0))\n",
    "        loss = ploss + nloss\n",
    "        loss.backward()\n",
    "        el.append(loss.item())\n",
    "        optimizer.step()\n",
    "        progress.update(1)\n",
    "\n",
    "    lh.append(sum(el) / len(el))\n",
    "\n",
    "progress.close()\n",
    "plt.plot(lh)\n",
    "plt.show()\n",
    "\n",
    "torch.save(model.state_dict(), 'complex_biob2.pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "Model(\n  (biobert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dff): Sequential(\n    (0): Linear(in_features=1536, out_features=3072, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.1, inplace=False)\n    (3): Linear(in_features=3072, out_features=768, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=768, out_features=256, bias=True)\n    (7): ReLU()\n    (8): Dropout(p=0.5, inplace=False)\n    (9): Linear(in_features=256, out_features=64, bias=True)\n    (10): ReLU()\n    (11): Dropout(p=0.5, inplace=False)\n    (12): Linear(in_features=64, out_features=1, bias=True)\n    (13): Sigmoid()\n  )\n)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model.load_state_dict(torch.load('complex_bio.pt'))\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muscle fiber hypertrophy ['hypertrophic', 'characteristic of part of', 'cell of skeletal muscle', 'has modifier', 'abnormal', 'increased amount', 'characteristic of', 'amyloid deposition', 'part of', 'nerve', 'has modifier', 'abnormal']\n",
      "Amyloidosis of peripheral nerves ['hypertrophic', 'characteristic of part of', 'cell of skeletal muscle', 'has modifier', 'abnormal', 'increased amount', 'characteristic of', 'amyloid deposition', 'part of', 'nerve', 'has modifier', 'abnormal']\n",
      "tensor([0.7600, 0.0228, 0.8211, 0.2107, 0.3271, 0.1553, 0.2601, 0.0024, 0.4524,\n",
      "        0.2386, 0.2107, 0.3271])\n",
      "tensor([0.0152, 0.0732, 0.4795, 0.5809, 0.6409, 0.6073, 0.5172, 0.8248, 0.6383,\n",
      "        0.7233, 0.5809, 0.6409])\n"
     ]
    }
   ],
   "source": [
    "print(x[0], c[0] + c[1])\n",
    "print(x[1], c[0] + c[1])\n",
    "print(model.sims(x[0], c[0] + c[1]))\n",
    "print(model.sims(x[1], c[0] + c[1]))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
